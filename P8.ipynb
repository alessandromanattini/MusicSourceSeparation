{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f74d155",
   "metadata": {},
   "source": [
    "**DSP DEMIXING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd09d18",
   "metadata": {},
   "source": [
    "Let's start with loading the dataset. The folder \"musdbhq_trimmed\" contains 30 seconda of all the tracks. Since we noticed that not all the stems of the tracks were non-silent in the first 30 seconds, we trimmed the dataset in order to retrieve 30 seconds of each track where every stem is non-silent, in order to have a more accurate measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4989528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, os, torchaudio\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the dataset from the musdb18hq_trimmed folder.\n",
    "    Each subfolder in the dataset corresponds to a song.\n",
    "    Each song contains multiple stems (e.g., mixture, drums, bass, etc.).\n",
    "    Returns:\n",
    "        dataset_dict (dict): A dictionary where keys are track folders and values are dictionaries of stems.\n",
    "    \"\"\"\n",
    "    dataset_dict = {}\n",
    "\n",
    "    for track_folder in tqdm.tqdm(os.listdir(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\")):\n",
    "        track_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder)\n",
    "        if not os.path.isdir(track_path):\n",
    "            continue\n",
    "\n",
    "        # Prepare a sub-dictionary for this song\n",
    "        stems_dict = {}\n",
    "        \n",
    "        for stem_name in [\"mixture\", \"drums\", \"bass\", \"vocals\", \"other\", \"new_mixture\"]:\n",
    "            file_path = os.path.abspath(os.path.join(track_path, f\"{stem_name}.wav\"))\n",
    "            \n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f\"Warning: file not found {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load full audio\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "            stems_dict[stem_name] = waveform\n",
    "            \n",
    "        dataset_dict[track_folder] = stems_dict\n",
    "        \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42af704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_dict = load_dataset()  \n",
    "\n",
    "print(\"Number of keys in dataset_dict:\", len(dataset_dict))\n",
    "\n",
    "# Check the first track folder and its contents\n",
    "first_track_folder = list(dataset_dict.keys())[0]\n",
    "print(\"First track folder:\", first_track_folder)\n",
    "print(\"Contents of the first track folder:\")\n",
    "for stem_name in dataset_dict[first_track_folder].keys():\n",
    "    print(f\" - {stem_name}: {dataset_dict[first_track_folder][stem_name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c08eb",
   "metadata": {},
   "source": [
    "Let's load all the mixtures in a list ***mixture_files[]***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all new_mixture.wav files \n",
    "mixture_files = []\n",
    "for track_folder in dataset_dict.keys():\n",
    "    new_mixture_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder, \"new_mixture.wav\")\n",
    "    if os.path.isfile(new_mixture_path):\n",
    "        mixture_files.append(new_mixture_path)\n",
    "    else:\n",
    "        print(f\"Warning: file not found {new_mixture_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3005ae5",
   "metadata": {},
   "source": [
    "Define the parameters of the STFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3825ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# STFT parameters\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "win = 'hann'\n",
    "\n",
    "# Initialize lists to store STFT results\n",
    "S_full_list = []\n",
    "phase_list = []\n",
    "\n",
    "# Loop through each mixture file and compute STFT\n",
    "for mixture_path in mixture_files:\n",
    "    # Carica l'audio dal file\n",
    "    audio, sr = librosa.load(mixture_path, sr=None)\n",
    "    # Calcola STFT\n",
    "    D = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length, window=win)\n",
    "    # Estrai modulo e fase\n",
    "    mag, phase = librosa.magphase(D)\n",
    "    \n",
    "    S_full_list.append(mag)\n",
    "    phase_list.append(phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d3cc2",
   "metadata": {},
   "source": [
    "Now we are going to define some functions:\n",
    "- drumExtraction\n",
    "- vocalExtraction\n",
    "- bassOtherExtraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87c58",
   "metadata": {},
   "source": [
    "**DRUMS EXTRACTION**\n",
    "(using HPSS):\n",
    "\n",
    "- STFT Magnitude Input: The function receives the magnitude (mix_mag) and phase (mix_phase) of the mixture’s STFT.\n",
    "\n",
    "- HPSS Decomposition: It utilizes the **Harmonic-Percussive Source Separation (HPSS)** algorithm to split the mixture’s magnitude into two components:\n",
    "    1) A ***harmonic component*** that captures the tonal content.\n",
    "    2) A ***percussive component*** that emphasizes transient, drum-like features.\n",
    "\n",
    "- Drums Reconstruction: The function then reconstructs the time-domain drums signal by combining the percussive component with the original phase information using the iSTFT.\n",
    "\n",
    "- Output: The result is a time-domain signal (drums) that represents the extracted percussive (drum) elements from the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drums extraction using HPSS\n",
    "import numpy as np\n",
    "\n",
    "def drums_extraction(mix_mag, mix_phase, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract drums from a mixture using Harmonic-Percussive Source Separation (HPSS).\n",
    "    \n",
    "    Parameters:\n",
    "        mixture (ndarray): The audio mixture.\n",
    "        sr (int): Sample rate of the audio.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        \n",
    "    Returns:\n",
    "        drums (ndarray): Extracted drums.\n",
    "    \"\"\"\n",
    "    # Decompose the mixture into harmonic and percussive components\n",
    "    S_harmonic, S_percussive = librosa.decompose.hpss(mix_mag)\n",
    "\n",
    "    # Reconstruct the drums using the percussive component and the original phase\n",
    "    y_drums = librosa.istft(S_percussive*mix_phase, hop_length=hop_length, win_length=n_fft, window=win)\n",
    "    \n",
    "    \n",
    "    return y_drums, S_harmonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18378f04",
   "metadata": {},
   "source": [
    "**Vocal Extraction** (using REPET-SIM):\n",
    "\n",
    "- STFT Magnitude Input: Similar to the drums extraction, this function takes the mixture’s magnitude (mix_mag) and phase (mix_phase) as input.\n",
    "\n",
    "- NN Filter Processing: The function applies a ***nearest-neighbor (nn) filter*** to the magnitude. This filter:\n",
    "    - Uses a median aggregation to estimate a smooth background signal.\n",
    "    - Operates with a cosine similarity metric and a time window (converted from 2.0 seconds into frames) to capture repeating patterns.\n",
    "\n",
    "- Filter Application: The filtered version (S_filter) is then constrained by taking the element-wise minimum with the original magnitude, ensuring that only components present in both are retained.\n",
    "\n",
    "- Soft Mask Creation: A soft mask is computed using librosa.util.softmask that emphasizes differences between the original magnitude and the filtered background. This mask is tuned (with a factor of 100 and power 2) to highlight vocal components.\n",
    "\n",
    "- Vocals Reconstruction: The function applies this mask to the original magnitude to produce a modified magnitude focused on the vocal content. It then reconstructs the time-domain vocal signal by combining this modified magnitude with the original phase via the iSTFT.\n",
    "\n",
    "- Output: The final output is a time-domain signal representing the extracted vocals from the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Vocal extraction using REPET-SIM\n",
    "def vocal_extraction(mix_mag, mix_phase, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract vocals from a mixture using REPET-SIM.\n",
    "    \"\"\"\n",
    "    # add small epsilon to avoid zero‐vectors in cosine similarity\n",
    "    eps = 1e-8\n",
    "    mix_mag_eps = mix_mag + eps\n",
    "\n",
    "    # suppress divide‐by‐zero / overflow warnings during nn_filter\n",
    "    \n",
    "    S_filter = librosa.decompose.nn_filter(\n",
    "        mix_mag_eps,\n",
    "        aggregate=np.median,\n",
    "        metric='cosine',\n",
    "        width=int(librosa.time_to_frames(2.0, sr=sr))\n",
    "    )\n",
    "    S_filter = np.minimum(S_filter, mix_mag_eps)\n",
    "    S_filter = np.nan_to_num(S_filter, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # create and sanitize soft mask\n",
    "    mask_vocal = librosa.util.softmask(\n",
    "        mix_mag_eps - S_filter,\n",
    "        100 * S_filter,\n",
    "        power=2\n",
    "    )\n",
    "    mask_vocal = np.nan_to_num(mask_vocal, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    S_vocal = mask_vocal * mix_mag_eps\n",
    "    y_vocals = librosa.istft(\n",
    "        S_vocal * mix_phase,\n",
    "        hop_length=hop_length,\n",
    "        win_length=n_fft,\n",
    "        window=win\n",
    "    )\n",
    "\n",
    "    return y_vocals, mask_vocal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279da80",
   "metadata": {},
   "source": [
    "**BASS & OTHERS EXTRACTION**\n",
    "\n",
    "For bass and other non-vocal components, the process begins with isolating the non-vocal harmonic residue by subtracting the estimated vocal contribution from the harmonic component. Frequency information is then used to create masks: one that selects frequencies below a certain threshold (250 Hz) to capture bass elements, and a complementary mask that selects the remaining frequencies for other elements. These masks are applied to the non-vocal residue, and by incorporating the original phase data, two distinct time-domain signals are reconstructed—one corresponding to the bass and the other to the rest of the non-vocal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8312908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bass and Other extraction\n",
    "def bass_other_extraction(S_harmonic, mask_vocal, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract bass and other components from a mixture using a simple thresholding method.\n",
    "    \n",
    "    Parameters:\n",
    "        S_harmonic (ndarray): The harmonic component of the mixture.\n",
    "        mask_vocal (ndarray): The vocal mask obtained from the vocal extraction.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.  \n",
    "    Outputs:\n",
    "        y_bass (ndarray): Extracted bass component.\n",
    "        y_other (ndarray): Extracted other component.   \n",
    "    \"\"\"\n",
    "    # Residuo armonico non-vocale\n",
    "    S_resid = S_harmonic - (mask_vocal * S_harmonic)\n",
    "\n",
    "    # Opzione A: filtro passa-basso per il basso (< 250 Hz)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    bass_mask = (freqs[:, None] < 250.0).astype(float)\n",
    "    other_mask = 1.0 - bass_mask\n",
    "\n",
    "    y_bass  = librosa.istft((S_resid * bass_mask) * phase, hop_length=hop_length, window=win)\n",
    "    y_other = librosa.istft((S_resid * other_mask) * phase, hop_length=hop_length, window=win)\n",
    "\n",
    "    return y_bass, y_other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c826f9",
   "metadata": {},
   "source": [
    "Now we are going to do the separation for just the first track of the dataset to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7217de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# Perform extraction for the first mixture and listen to the results\n",
    "mixture_path = mixture_files[0]\n",
    "mixture, sr = librosa.load(mixture_path, sr=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1753369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# Load the first mixture of the dataset and play it\n",
    "mixture, sr = librosa.load(mixture_path, sr=None)\n",
    "print(\"Mixture:\")\n",
    "ipd.Audio(mixture, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpss_stereo.py\n",
    "# HPSS “from scratch” su segnali stereo\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.ndimage\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def hpss_stereo(\n",
    "    path_in: str,\n",
    "    *,\n",
    "    sr: int = 44_100,\n",
    "    n_fft: int = 4096,\n",
    "    hop: int = 1024,\n",
    "    harmonic_filt: int = 31,\n",
    "    percussive_filt: int = 31,\n",
    "    mask: str = \"soft\",        # \"soft\" oppure \"binary\"\n",
    "    power: float = 2.0,        # =2 ⇒ spettro di potenza; =1 ⇒ magnitudine\n",
    "    out_harm: str | None = None,\n",
    "    out_perc: str | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    HPSS per file stereo: restituisce due array (shape=(2, n_samples))\n",
    "    contenenti i segnali armonico e percussivo per i canali [L, R].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_in         : percorso del file stereo (wav, flac, mp3,…)\n",
    "    sr              : sample rate di lavoro\n",
    "    n_fft, hop      : parametri STFT\n",
    "    harmonic_filt   : lunghezza filtro orizzontale (frames)\n",
    "    percussive_filt : lunghezza filtro verticale (bin freq)\n",
    "    mask            : \"soft\" o \"binary\"\n",
    "    power           : esponente per lo spettro (1=magn, 2=potenza)\n",
    "    out_harm/out_perc : percorsi opzionali per salvare gli stem stereo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_h, y_p : np.ndarray\n",
    "        Array shape=(2, n_samples): [canale_L, canale_R] separati.\n",
    "    \"\"\"\n",
    "    # 1. carica stereo\n",
    "    y, _ = librosa.load(path_in, sr=sr, mono=False)  # shape (2, n)\n",
    "    if y.ndim == 1:\n",
    "        # se mono, duplichiamo per mantenere shape coerente\n",
    "        y = np.vstack([y, y])\n",
    "    n_channels, n_samples = y.shape\n",
    "\n",
    "    # container per gli stem\n",
    "    y_h = np.zeros_like(y)\n",
    "    y_p = np.zeros_like(y)\n",
    "\n",
    "    # 2–6. per ciascun canale, esegui la pipeline HPSS\n",
    "    for ch in range(n_channels):\n",
    "        X = librosa.stft(y[ch], n_fft=n_fft, hop_length=hop, window=\"hann\")\n",
    "        mag = np.abs(X)\n",
    "        Y = mag**power\n",
    "\n",
    "        # median filtering anisotropo\n",
    "        Y_h = scipy.ndimage.median_filter(Y, size=(1, harmonic_filt))\n",
    "        Y_p = scipy.ndimage.median_filter(Y, size=(percussive_filt, 1))\n",
    "\n",
    "        # creazione maschere\n",
    "        if mask == \"binary\":\n",
    "            Mh = (Y_h >= Y_p).astype(np.float32)\n",
    "            Mp = 1.0 - Mh\n",
    "        else:  # soft\n",
    "            eps = 1e-10\n",
    "            denom = Y_h + Y_p + eps\n",
    "            Mh = Y_h / denom\n",
    "            Mp = Y_p / denom\n",
    "\n",
    "        # applicazione maschere e ricostruzione\n",
    "        X_h = X * Mh\n",
    "        X_p = X * Mp\n",
    "        y_h[ch] = librosa.istft(X_h, hop_length=hop, window=\"hann\", length=n_samples)\n",
    "        y_p[ch] = librosa.istft(X_p, hop_length=hop, window=\"hann\", length=n_samples)\n",
    "\n",
    "    return y_h, y_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and save the two components\n",
    "harmonic_comp, percussive_comp = hpss_stereo(\n",
    "    path_in=mixture_path,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop=512,\n",
    "    harmonic_filt=31,\n",
    "    percussive_filt=31,\n",
    "    mask=\"soft\",\n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Percussive Component:\")\n",
    "print(percussive_comp.shape)\n",
    "display(ipd.Audio(percussive_comp, rate=44100))\n",
    "print(\"Harmonic Component:\")\n",
    "display(ipd.Audio(harmonic_comp, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_comp, percussive_comp = hpss_stereo(\n",
    "    path_in=mixture_path,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop=512,\n",
    "    harmonic_filt=51,\n",
    "    percussive_filt=101,\n",
    "    mask=\"binary\",\n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Percussive Component:\")\n",
    "print(percussive_comp.shape)\n",
    "display(ipd.Audio(percussive_comp, rate=44100))\n",
    "print(\"Harmonic Component:\")\n",
    "display(ipd.Audio(harmonic_comp, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c98459",
   "metadata": {},
   "source": [
    "**REPET_SIM VOCAL EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.ndimage\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "def repet_separate_stereo(\n",
    "    y: str,\n",
    "    *,\n",
    "    sr: int = 44100,\n",
    "    n_fft: int = 2048,\n",
    "    hop_length: int = 512,\n",
    "    mask_type: str = \"soft\",  # \"soft\" or \"binary\"\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Separate vocals and music from a stereo mix using the REPET algorithm,\n",
    "    preserving stereo image by computing mask on the mid-channel and\n",
    "    applying smoothed channel-specific masks to L and R.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_in : str\n",
    "        Path to input stereo audio file.\n",
    "    sr : int\n",
    "        Sampling rate for loading and processing.\n",
    "    n_fft : int\n",
    "        FFT window size for STFT.\n",
    "    hop_length : int\n",
    "        Hop length between STFT frames.\n",
    "    mask_type : {\"soft\", \"binary\"}\n",
    "        Type of mask: \"soft\" for proportional, \"binary\" for hard.\n",
    "    out_vocal : str | None\n",
    "        Optional path to save separated vocal track (WAV), stereo.\n",
    "    out_music : str | None\n",
    "        Optional path to save separated music track (WAV), stereo.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocals : np.ndarray\n",
    "        Separated vocal signal, shape=(2, n_samples).\n",
    "    music  : np.ndarray\n",
    "        Separated music signal, shape=(2, n_samples).\n",
    "    \"\"\"\n",
    "    # 1. Load stereo audio\n",
    "    # y, _ = librosa.load(path_in, sr=sr, mono=False)\n",
    "    #display(ipd.Audio(y, rate=sr))\n",
    "    if y.ndim == 1:\n",
    "        y = np.vstack([y, y])\n",
    "    n_ch, n_samples = y.shape\n",
    "\n",
    "    # 2. Compute mid-channel for mask estimation\n",
    "    y_mid = np.mean(y, axis=0)\n",
    "\n",
    "    # 3. STFT on mid-channel\n",
    "    X_mid = librosa.stft(y_mid, n_fft=n_fft, hop_length=hop_length, window='hann', center=True)\n",
    "    V_mid = np.abs(X_mid) ** 2\n",
    "\n",
    "    # 4. Estimate repeating period using onset envelope\n",
    "    onset_env = librosa.onset.onset_strength(y=y_mid, sr=sr, hop_length=hop_length)\n",
    "    tempos = librosa.beat.tempo(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "    if len(tempos) == 0:\n",
    "        raise ValueError(\"Could not estimate tempo. Check audio quality.\")\n",
    "    tempo = float(tempos[0])\n",
    "    period = int(round(60 * sr / (tempo * hop_length)))\n",
    "    if period < 1:\n",
    "        raise ValueError(\"Estimated period too small. Check audio or parameters.\")\n",
    "    print(f\"Estimated tempo: {tempo:.2f} BPM, repeating period: {period} frames\")\n",
    "\n",
    "    # 5. Build repeating pattern W\n",
    "    n_bins, n_frames = V_mid.shape\n",
    "    n_segments = n_frames // period\n",
    "    if n_segments < 2:\n",
    "        raise ValueError(\"Not enough repeating segments. Try a smaller period or longer audio.\")\n",
    "    V_trim = V_mid[:, :n_segments * period]\n",
    "    segments = V_trim.reshape(n_bins, period, n_segments)\n",
    "    W = np.median(segments, axis=2)\n",
    "\n",
    "    # 6. Expand W to full length\n",
    "    W_full = np.tile(W, (1, n_segments))\n",
    "    tail = n_frames - W_full.shape[1]\n",
    "    if tail > 0:\n",
    "        W_full = np.hstack((W_full, W[:, :tail]))\n",
    "\n",
    "    # 7. Separate per channel with smoothing\n",
    "    music = np.zeros((n_ch, n_samples))\n",
    "    vocals = np.zeros((n_ch, n_samples))\n",
    "    eps = 1e-10\n",
    "    for ch in range(n_ch):\n",
    "        # STFT per channel\n",
    "        X = librosa.stft(y[ch], n_fft=n_fft, hop_length=hop_length, window='hann', center=True)\n",
    "        V = np.abs(X)**2\n",
    "\n",
    "        # Mask creation per channel\n",
    "        if mask_type == \"binary\":\n",
    "            M = (V >= W_full).astype(float)\n",
    "        else:\n",
    "            M = W_full / (V + eps)\n",
    "        M = np.clip(M, 0, 1)\n",
    "\n",
    "        # Smoothing mask to reduce artefacts\n",
    "        mask_amp = np.sqrt(M)\n",
    "        mask_amp = scipy.ndimage.median_filter(mask_amp, size=(3, 3))\n",
    "\n",
    "        # Apply mask and reconstruct music\n",
    "        X_music = X * mask_amp\n",
    "        y_music = librosa.istft(X_music, hop_length=hop_length, window='hann', center=True, length=n_samples)\n",
    "        # Normalize to avoid clipping\n",
    "        y_music = librosa.util.normalize(y_music)\n",
    "        music[ch] = y_music\n",
    "        vocals[ch] = y[ch] - y_music\n",
    "\n",
    "\n",
    "    return vocals, music\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal, music = repet_separate_stereo(\n",
    "    y=mixture,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    mask_type=\"soft\"\n",
    ")\n",
    "other = mixture - vocal - music\n",
    "print(\"Mixture:\")\n",
    "display(ipd.Audio(mixture, rate=44100))\n",
    "print(\"Difference between original and music component:\")\n",
    "display(ipd.Audio(other, rate=44100))\n",
    "\n",
    "# output_path = Path(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE\")\n",
    "# output_other= output_path / \"other.wav\"\n",
    "# sf.write(output_other, other.T, 44100)\n",
    "\n",
    "# _, drums = hpss_stereo(\n",
    "#     path_in=output_other,\n",
    "#     sr=44100,\n",
    "#     n_fft=2048,\n",
    "#     hop=512,\n",
    "#     harmonic_filt=31,\n",
    "#     percussive_filt=31,\n",
    "#     mask=\"soft\",\n",
    "#     power=2.0\n",
    "# )\n",
    "\n",
    "# print(\"Drums Component:\")\n",
    "\n",
    "# display(ipd.Audio(drums, rate=44100))\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Vocal Component:\")\n",
    "display(ipd.Audio(vocal, rate=44100))\n",
    "print(\"Music Component:\")\n",
    "display(ipd.Audio(music, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780b218",
   "metadata": {},
   "source": [
    "HPSS+REPET-SIM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def separate_sources(\n",
    "    mixture_path,\n",
    "    sr=None,\n",
    "    hpss_margin=1.0,\n",
    "    bass_cutoff=200,\n",
    "    bass_order=4,\n",
    "    nn_width_sec=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Load an audio file and separate it into drums, bass, vocals and other.\n",
    "    Returns a dict of numpy arrays (all mono).\n",
    "    \"\"\"\n",
    "    # 1) load & mono\n",
    "    y, sr = librosa.load(mixture_path, sr=sr)\n",
    "    if y.ndim > 1:\n",
    "        y = librosa.to_mono(y)\n",
    "\n",
    "    # 2) HPSS for initial drums extraction\n",
    "    D = librosa.stft(y)\n",
    "    D_harm, D_perc = librosa.decompose.hpss(D, margin=hpss_margin, power=2.0)\n",
    "    y_drums_initial = librosa.istft(D_perc, length=len(y))\n",
    "    y_harmonic = librosa.istft(D_harm, length=len(y))\n",
    "    \n",
    "    # 3) Apply REPET-SIM to drums to remove vocal residuals\n",
    "    S_drums, phase_drums = librosa.magphase(librosa.stft(y_drums_initial))\n",
    "    width = int(librosa.time_to_frames(nn_width_sec, sr=sr))\n",
    "    S_filter = librosa.decompose.nn_filter(\n",
    "        S_drums, aggregate=np.median, metric='cosine', width=width\n",
    "    )\n",
    "    # Extract vocal residuals from drums (to be discarded)\n",
    "    vocal_residuals_in_drums = np.clip(S_drums - S_filter, 0, None)\n",
    "\n",
    "    # Listen to the vocal residuals in drums\n",
    "    y_vocal_residuals_in_drums = librosa.istft(vocal_residuals_in_drums * phase_drums, length=len(y))\n",
    "    print(\"Vocal residuals in drums:\")\n",
    "    ipd.display(ipd.Audio(y_vocal_residuals_in_drums, rate=sr))\n",
    "\n",
    "    # Listen to S_drums before cleaning\n",
    "    y_drums_before_cleaning = librosa.istft(S_drums * phase_drums, length=len(y))\n",
    "    print(\"Drums before cleaning:\")\n",
    "    ipd.display(ipd.Audio(y_drums_before_cleaning, rate=sr))\n",
    "\n",
    "    # Listen to S_filter\n",
    "    y_filter = librosa.istft(S_filter * phase_drums, length=len(y))\n",
    "    print(\"Filter applied to drums:\")\n",
    "    ipd.display(ipd.Audio(y_filter, rate=sr))\n",
    "\n",
    "    # Clean drums = original drums - vocal residuals\n",
    "    S_drums_clean = S_drums - vocal_residuals_in_drums\n",
    "    y_drums = librosa.istft(S_drums_clean * phase_drums, length=len(y))\n",
    "\n",
    "    # 4) low-pass for bass\n",
    "    nyq = 0.5 * sr\n",
    "    b, a = scipy.signal.butter(bass_order, bass_cutoff/nyq, btype='low')\n",
    "    y_bass = scipy.signal.lfilter(b, a, y_harmonic)\n",
    "\n",
    "    # 5) NN-median filter for vocals from harmonic component\n",
    "    S_full, phase = librosa.magphase(librosa.stft(y_harmonic))\n",
    "    S_filter = librosa.decompose.nn_filter(\n",
    "        S_full, aggregate=np.median, metric='cosine', width=width\n",
    "    )\n",
    "    mask_voc = np.clip(S_full - S_filter, 0, None)\n",
    "    y_vocals = librosa.istft(mask_voc * phase, length=len(y))\n",
    "\n",
    "    # 6) residual \"other\"\n",
    "    y_other = y - y_drums - y_bass - y_vocals\n",
    "\n",
    "    return {\n",
    "        'drums':   y_drums,\n",
    "        'bass':    y_bass,\n",
    "        'vocals':  y_vocals,\n",
    "        'other':   y_other,\n",
    "        'sr': sr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85ac2e",
   "metadata": {},
   "source": [
    "Single Track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ce61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = separate_sources(mixture_path)\n",
    "drums  = sources['drums']\n",
    "bass   = sources['bass']\n",
    "vocals = sources['vocals']\n",
    "other  = sources['other']\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Drums Component:\")\n",
    "display(ipd.Audio(drums, rate=sources['sr']))\n",
    "print(\"Bass Component:\")\n",
    "display(ipd.Audio(bass, rate=sources['sr']))\n",
    "print(\"Vocals Component:\")\n",
    "display(ipd.Audio(vocals, rate=sources['sr']))\n",
    "print(\"Other Component:\")\n",
    "display(ipd.Audio(other, rate=sources['sr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cfa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform extraction for the first mixture and listen to the results\n",
    "mixture_path = mixture_files[49]\n",
    "mixture, sr = librosa.load(mixture_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf32b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def separate_sources_v2( # Rinomino la funzione per chiarezza\n",
    "    mixture_path,\n",
    "    sr=None,\n",
    "    hpss_margin=1.0, # Leggermente aumentato come punto di partenza\n",
    "    hpss_power=2.0,\n",
    "    # Parametri per la pulizia delle voci dalla batteria\n",
    "    drum_clean_vocal_freq_min=300.0,  # Hz, inizio range vocale da attenuare\n",
    "    drum_clean_vocal_freq_max=3000.0,  # Hz, fine range vocale da attenuare\n",
    "    drum_clean_vocal_atten_factor=0.4, # Fattore di attenuazione (0.0 = muto, 1.0 = nessun cambiamento)\n",
    "    # Parametri per il basso\n",
    "    bass_cutoff=200.0,\n",
    "    bass_order=4,\n",
    "    # Parametri per la separazione vocale principale\n",
    "    nn_width_vocals_sec=2.0 # Aumentato per una migliore separazione vocale\n",
    "):\n",
    "    \"\"\"\n",
    "    Carica un file audio e lo separa in batteria, basso, voci e altro,\n",
    "    con un passaggio dedicato per pulire i residui vocali dalla batteria.\n",
    "    Restituisce un dizionario di array numpy (tutti mono).\n",
    "    \"\"\"\n",
    "    # 1) Caricamento e conversione in mono\n",
    "    y, sr_loaded = librosa.load(mixture_path, sr=sr)\n",
    "    if sr is None: # Se sr non era specificato, usa quello del file\n",
    "        sr = sr_loaded\n",
    "        \n",
    "    if y.ndim > 1:\n",
    "        y = librosa.to_mono(y)\n",
    "\n",
    "    # 2) HPSS per l'estrazione iniziale di batteria e componente armonica\n",
    "    # Calcola lo STFT del mix originale\n",
    "    D_mixture = librosa.stft(y)\n",
    "    n_fft = (D_mixture.shape[0] - 1) * 2 # Infer n_fft dallo spettrogramma\n",
    "    \n",
    "    D_harmonic_mixture, D_percussive_mixture = librosa.decompose.hpss(\n",
    "        D_mixture, \n",
    "        margin=hpss_margin, \n",
    "        power=hpss_power\n",
    "    )\n",
    "    \n",
    "    # Componente armonica generale (verrà usata per basso e voci)\n",
    "    y_harmonic_overall = librosa.istft(D_harmonic_mixture, length=len(y))\n",
    "\n",
    "    # --- 3) Pulizia della Batteria dai Residui Vocali ---\n",
    "    # Partiamo da D_percussive_mixture (lo spettrogramma della batteria da HPSS)\n",
    "    D_perc_mag, D_perc_phase = librosa.magphase(D_percussive_mixture)\n",
    "    \n",
    "    # Crea una copia della magnitudine per la modifica\n",
    "    D_perc_mag_cleaned = np.copy(D_perc_mag)\n",
    "    \n",
    "    # Ottieni le frequenze corrispondenti ai bin dello STFT\n",
    "    frequencies = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    \n",
    "    # Applica l'attenuazione nel range di frequenza vocale definito\n",
    "    for i, freq_bin in enumerate(frequencies):\n",
    "        if drum_clean_vocal_freq_min <= freq_bin <= drum_clean_vocal_freq_max:\n",
    "            D_perc_mag_cleaned[i, :] *= drum_clean_vocal_atten_factor\n",
    "            \n",
    "    # Ricostruisci lo spettrogramma della batteria pulita\n",
    "    D_drums_cleaned_stft = D_perc_mag_cleaned * D_perc_phase\n",
    "    y_drums = librosa.istft(D_drums_cleaned_stft, length=len(y))\n",
    "\n",
    "    # --- 4) Estrazione del Basso dalla componente armonica generale ---\n",
    "    # Applica un filtro passa-basso a y_harmonic_overall\n",
    "    nyquist = 0.5 * sr\n",
    "    # Assicurati che bass_cutoff sia sotto la frequenza di Nyquist\n",
    "    actual_bass_cutoff = min(bass_cutoff, nyquist - 1) # -1 per sicurezza\n",
    "    if actual_bass_cutoff <= 0:\n",
    "        print(f\"Attenzione: bass_cutoff ({bass_cutoff} Hz) non valido con sr={sr} Hz. Il basso non verrà filtrato.\")\n",
    "        y_bass = np.zeros_like(y_harmonic_overall) # o gestisci diversamente\n",
    "    else:\n",
    "        b, a = scipy.signal.butter(bass_order, actual_bass_cutoff / nyquist, btype='low', analog=False)\n",
    "        y_bass = scipy.signal.lfilter(b, a, y_harmonic_overall)\n",
    "\n",
    "    # --- 5) Estrazione delle Voci dalla componente armonica generale ---\n",
    "    # Nota: per una migliore separazione, si potrebbe sottrarre il basso stimato\n",
    "    # da y_harmonic_overall prima di cercare le voci, ma per semplicità usiamo y_harmonic_overall.\n",
    "    # y_harmonic_minus_bass = y_harmonic_overall - y_bass # Opzionale, potrebbe aiutare\n",
    "    \n",
    "    S_harmonic_overall, phase_harmonic_overall = librosa.magphase(librosa.stft(y_harmonic_overall)) # o di y_harmonic_minus_bass\n",
    "    \n",
    "    width_vocals_frames = int(librosa.time_to_frames(nn_width_vocals_sec, sr=sr, n_fft=n_fft))\n",
    "    \n",
    "    S_instrumental_repeating = librosa.decompose.nn_filter(\n",
    "        S_harmonic_overall, \n",
    "        aggregate=np.median, \n",
    "        metric='cosine', \n",
    "        width=width_vocals_frames\n",
    "    )\n",
    "    \n",
    "    # La maschera per le voci è ciò che NON è ripetitivo nella componente armonica\n",
    "    S_vocals_mag = np.clip(S_harmonic_overall - S_instrumental_repeating, 0, None)\n",
    "    \n",
    "    y_vocals = librosa.istft(S_vocals_mag * phase_harmonic_overall, length=len(y))\n",
    "\n",
    "    # --- 6) Calcolo del Residuo \"Other\" ---\n",
    "    # Sottrai le componenti stimate dal mix originale\n",
    "    y_other = y - y_drums - y_bass - y_vocals\n",
    "\n",
    "    return {\n",
    "        'drums': y_drums,\n",
    "        'bass': y_bass,\n",
    "        'vocals': y_vocals,\n",
    "        'other': y_other,\n",
    "        'sr': sr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77f4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = separate_sources_v2(mixture_path)\n",
    "drums  = sources['drums']\n",
    "bass   = sources['bass']\n",
    "vocals = sources['vocals']\n",
    "other  = sources['other']\n",
    "\n",
    "# play mixture\n",
    "print(\"Mixture:\")\n",
    "display(ipd.Audio(mixture, rate=sr))\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Drums Component:\")\n",
    "display(ipd.Audio(drums, rate=sources['sr']))\n",
    "print(\"Bass Component:\")\n",
    "display(ipd.Audio(bass, rate=sources['sr']))\n",
    "print(\"Vocals Component:\")\n",
    "display(ipd.Audio(vocals, rate=sources['sr']))\n",
    "print(\"Other Component:\")\n",
    "display(ipd.Audio(other, rate=sources['sr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb2488d",
   "metadata": {},
   "source": [
    "Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56543dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchmetrics import SignalDistortionRatio\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99215887",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_metric = SignalDistortionRatio()\n",
    "\n",
    "# Calculate average SDR for each stem across all tracks\n",
    "stems = ['drums', 'vocals', 'bass', 'other']\n",
    "average_sdr = {}\n",
    "\n",
    "for stem in stems:\n",
    "    stem_sdrs = []\n",
    "    for track_folder in dataset_dict.keys():\n",
    "        mixture_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder, \"new_mixture.wav\")\n",
    "        separated_sources = separate_sources(mixture_path)\n",
    "        \n",
    "        if stem in dataset_dict[track_folder]:\n",
    "            ref_stem = dataset_dict[track_folder][stem]\n",
    "            est_stem = separated_sources[stem]\n",
    "            \n",
    "            ref_tensor = torch.tensor(ref_stem, dtype=torch.float32)\n",
    "            est_tensor = torch.tensor(est_stem, dtype=torch.float32)\n",
    "            \n",
    "            if ref_tensor.dim() == 2 and est_tensor.dim() == 1:\n",
    "                ref_tensor = torch.mean(ref_tensor, dim=0)\n",
    "            elif ref_tensor.dim() == 1 and est_tensor.dim() == 2:\n",
    "                est_tensor = torch.mean(est_tensor, dim=0)\n",
    "            elif ref_tensor.dim() == 2 and est_tensor.dim() == 2:\n",
    "                ref_tensor = torch.mean(ref_tensor, dim=0)\n",
    "                est_tensor = torch.mean(est_tensor, dim=0)\n",
    "            \n",
    "            min_len = min(len(ref_tensor), len(est_tensor))\n",
    "            ref_tensor = ref_tensor[:min_len]\n",
    "            est_tensor = est_tensor[:min_len]\n",
    "            \n",
    "            sdr_value = sdr_metric(est_tensor, ref_tensor)\n",
    "            stem_sdrs.append(sdr_value.item())\n",
    "    \n",
    "    average_sdr[stem] = np.mean(stem_sdrs) if stem_sdrs else 0\n",
    "\n",
    "# Create bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(average_sdr.keys(), average_sdr.values(), color=['blue', 'green', 'red', 'orange'])\n",
    "plt.xlabel('Stem Category')\n",
    "plt.ylabel('Average SDR (dB)')\n",
    "plt.title('Average SDR Performance by Stem Category')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, value in zip(bars, average_sdr.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average SDR by stem:\")\n",
    "for stem, avg_sdr in average_sdr.items():\n",
    "    print(f\"{stem}: {avg_sdr:.4f} dB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2563f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdr_metric = SignalDistortionRatio()\n",
    "\n",
    "# Calculate average SDR for each stem across all tracks\n",
    "stems = ['drums', 'vocals', 'bass', 'other']\n",
    "average_sdr = {}\n",
    "\n",
    "for stem in stems:\n",
    "    stem_sdrs = []\n",
    "    for track_folder in dataset_dict.keys():\n",
    "        mixture_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder, \"new_mixture.wav\")\n",
    "        separated_sources = separate_sources_v2(mixture_path)\n",
    "        \n",
    "        if stem in dataset_dict[track_folder]:\n",
    "            ref_stem = dataset_dict[track_folder][stem]\n",
    "            est_stem = separated_sources[stem]\n",
    "            \n",
    "            ref_tensor = torch.tensor(ref_stem, dtype=torch.float32)\n",
    "            est_tensor = torch.tensor(est_stem, dtype=torch.float32)\n",
    "            \n",
    "            if ref_tensor.dim() == 2 and est_tensor.dim() == 1:\n",
    "                ref_tensor = torch.mean(ref_tensor, dim=0)\n",
    "            elif ref_tensor.dim() == 1 and est_tensor.dim() == 2:\n",
    "                est_tensor = torch.mean(est_tensor, dim=0)\n",
    "            elif ref_tensor.dim() == 2 and est_tensor.dim() == 2:\n",
    "                ref_tensor = torch.mean(ref_tensor, dim=0)\n",
    "                est_tensor = torch.mean(est_tensor, dim=0)\n",
    "            \n",
    "            min_len = min(len(ref_tensor), len(est_tensor))\n",
    "            ref_tensor = ref_tensor[:min_len]\n",
    "            est_tensor = est_tensor[:min_len]\n",
    "            \n",
    "            sdr_value = sdr_metric(est_tensor, ref_tensor)\n",
    "            stem_sdrs.append(sdr_value.item())\n",
    "    \n",
    "    average_sdr[stem] = np.mean(stem_sdrs) if stem_sdrs else 0\n",
    "\n",
    "# Create bar plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(average_sdr.keys(), average_sdr.values(), color=['blue', 'green', 'red', 'orange'])\n",
    "plt.xlabel('Stem Category')\n",
    "plt.ylabel('Average SDR (dB)')\n",
    "plt.title('Average SDR Performance by Stem Category')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar, value in zip(bars, average_sdr.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{value:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Average SDR by stem:\")\n",
    "for stem, avg_sdr in average_sdr.items():\n",
    "    print(f\"{stem}: {avg_sdr:.4f} dB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
