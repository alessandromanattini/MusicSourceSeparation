{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f74d155",
   "metadata": {},
   "source": [
    "**DSP DEMIXING**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd09d18",
   "metadata": {},
   "source": [
    "Let's start with loading the dataset. The folder \"musdbhq_trimmed\" contains 30 seconda of all the tracks. Since we noticed that not all the stems of the tracks were non-silent in the first 30 seconds, we trimmed the dataset in order to retrieve 30 seconds of each track where every stem is non-silent, in order to have a more accurate measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4989528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm, os, torchaudio\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    Load the dataset from the musdb18hq_trimmed folder.\n",
    "    Each subfolder in the dataset corresponds to a song.\n",
    "    Each song contains multiple stems (e.g., mixture, drums, bass, etc.).\n",
    "    Returns:\n",
    "        dataset_dict (dict): A dictionary where keys are track folders and values are dictionaries of stems.\n",
    "    \"\"\"\n",
    "    dataset_dict = {}\n",
    "\n",
    "    for track_folder in tqdm.tqdm(os.listdir(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\")):\n",
    "        track_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder)\n",
    "        if not os.path.isdir(track_path):\n",
    "            continue\n",
    "\n",
    "        # Prepare a sub-dictionary for this song\n",
    "        stems_dict = {}\n",
    "        \n",
    "        for stem_name in [\"mixture\", \"drums\", \"bass\", \"vocals\", \"other\", \"new_mixture\"]:\n",
    "            file_path = os.path.abspath(os.path.join(track_path, f\"{stem_name}.wav\"))\n",
    "            \n",
    "            if not os.path.isfile(file_path):\n",
    "                print(f\"Warning: file not found {file_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load full audio\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "            stems_dict[stem_name] = waveform\n",
    "            \n",
    "        dataset_dict[track_folder] = stems_dict\n",
    "        \n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42af704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_dict = load_dataset()  \n",
    "\n",
    "print(\"Number of keys in dataset_dict:\", len(dataset_dict))\n",
    "\n",
    "# Check the first track folder and its contents\n",
    "first_track_folder = list(dataset_dict.keys())[0]\n",
    "print(\"First track folder:\", first_track_folder)\n",
    "print(\"Contents of the first track folder:\")\n",
    "for stem_name in dataset_dict[first_track_folder].keys():\n",
    "    print(f\" - {stem_name}: {dataset_dict[first_track_folder][stem_name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c08eb",
   "metadata": {},
   "source": [
    "Let's load all the mixtures in a list ***mixture_files[]***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all new_mixture.wav files \n",
    "mixture_files = []\n",
    "for track_folder in dataset_dict.keys():\n",
    "    new_mixture_path = os.path.join(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\", track_folder, \"new_mixture.wav\")\n",
    "    if os.path.isfile(new_mixture_path):\n",
    "        mixture_files.append(new_mixture_path)\n",
    "    else:\n",
    "        print(f\"Warning: file not found {new_mixture_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3005ae5",
   "metadata": {},
   "source": [
    "Define the parameters of the STFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3825ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# STFT parameters\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "win = 'hann'\n",
    "\n",
    "# Initialize lists to store STFT results\n",
    "S_full_list = []\n",
    "phase_list = []\n",
    "\n",
    "# Loop through each mixture file and compute STFT\n",
    "for mixture_path in mixture_files:\n",
    "    # Carica l'audio dal file\n",
    "    audio, sr = librosa.load(mixture_path, sr=None)\n",
    "    # Calcola STFT\n",
    "    D = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length, window=win)\n",
    "    # Estrai modulo e fase\n",
    "    mag, phase = librosa.magphase(D)\n",
    "    \n",
    "    S_full_list.append(mag)\n",
    "    phase_list.append(phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d3cc2",
   "metadata": {},
   "source": [
    "Now we are going to define some functions:\n",
    "- drumExtraction\n",
    "- vocalExtraction\n",
    "- bassOtherExtraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c87c58",
   "metadata": {},
   "source": [
    "**DRUMS EXTRACTION**\n",
    "(using HPSS):\n",
    "\n",
    "- STFT Magnitude Input: The function receives the magnitude (mix_mag) and phase (mix_phase) of the mixture’s STFT.\n",
    "\n",
    "- HPSS Decomposition: It utilizes the **Harmonic-Percussive Source Separation (HPSS)** algorithm to split the mixture’s magnitude into two components:\n",
    "    1) A ***harmonic component*** that captures the tonal content.\n",
    "    2) A ***percussive component*** that emphasizes transient, drum-like features.\n",
    "\n",
    "- Drums Reconstruction: The function then reconstructs the time-domain drums signal by combining the percussive component with the original phase information using the iSTFT.\n",
    "\n",
    "- Output: The result is a time-domain signal (drums) that represents the extracted percussive (drum) elements from the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drums extraction using HPSS\n",
    "import numpy as np\n",
    "\n",
    "def drums_extraction(mix_mag, mix_phase, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract drums from a mixture using Harmonic-Percussive Source Separation (HPSS).\n",
    "    \n",
    "    Parameters:\n",
    "        mixture (ndarray): The audio mixture.\n",
    "        sr (int): Sample rate of the audio.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.\n",
    "        \n",
    "    Returns:\n",
    "        drums (ndarray): Extracted drums.\n",
    "    \"\"\"\n",
    "    # Decompose the mixture into harmonic and percussive components\n",
    "    S_harmonic, S_percussive = librosa.decompose.hpss(mix_mag)\n",
    "\n",
    "    # Reconstruct the drums using the percussive component and the original phase\n",
    "    y_drums = librosa.istft(S_percussive*mix_phase, hop_length=hop_length, win_length=n_fft, window=win)\n",
    "    \n",
    "    \n",
    "    return y_drums, S_harmonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18378f04",
   "metadata": {},
   "source": [
    "**Vocal Extraction** (using REPET-SIM):\n",
    "\n",
    "- STFT Magnitude Input: Similar to the drums extraction, this function takes the mixture’s magnitude (mix_mag) and phase (mix_phase) as input.\n",
    "\n",
    "- NN Filter Processing: The function applies a ***nearest-neighbor (nn) filter*** to the magnitude. This filter:\n",
    "    - Uses a median aggregation to estimate a smooth background signal.\n",
    "    - Operates with a cosine similarity metric and a time window (converted from 2.0 seconds into frames) to capture repeating patterns.\n",
    "\n",
    "- Filter Application: The filtered version (S_filter) is then constrained by taking the element-wise minimum with the original magnitude, ensuring that only components present in both are retained.\n",
    "\n",
    "- Soft Mask Creation: A soft mask is computed using librosa.util.softmask that emphasizes differences between the original magnitude and the filtered background. This mask is tuned (with a factor of 100 and power 2) to highlight vocal components.\n",
    "\n",
    "- Vocals Reconstruction: The function applies this mask to the original magnitude to produce a modified magnitude focused on the vocal content. It then reconstructs the time-domain vocal signal by combining this modified magnitude with the original phase via the iSTFT.\n",
    "\n",
    "- Output: The final output is a time-domain signal representing the extracted vocals from the mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Vocal extraction using REPET-SIM\n",
    "def vocal_extraction(mix_mag, mix_phase, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract vocals from a mixture using REPET-SIM.\n",
    "    \"\"\"\n",
    "    # add small epsilon to avoid zero‐vectors in cosine similarity\n",
    "    eps = 1e-8\n",
    "    mix_mag_eps = mix_mag + eps\n",
    "\n",
    "    # suppress divide‐by‐zero / overflow warnings during nn_filter\n",
    "    \n",
    "    S_filter = librosa.decompose.nn_filter(\n",
    "        mix_mag_eps,\n",
    "        aggregate=np.median,\n",
    "        metric='cosine',\n",
    "        width=int(librosa.time_to_frames(2.0, sr=sr))\n",
    "    )\n",
    "    S_filter = np.minimum(S_filter, mix_mag_eps)\n",
    "    S_filter = np.nan_to_num(S_filter, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # create and sanitize soft mask\n",
    "    mask_vocal = librosa.util.softmask(\n",
    "        mix_mag_eps - S_filter,\n",
    "        100 * S_filter,\n",
    "        power=2\n",
    "    )\n",
    "    mask_vocal = np.nan_to_num(mask_vocal, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    S_vocal = mask_vocal * mix_mag_eps\n",
    "    y_vocals = librosa.istft(\n",
    "        S_vocal * mix_phase,\n",
    "        hop_length=hop_length,\n",
    "        win_length=n_fft,\n",
    "        window=win\n",
    "    )\n",
    "\n",
    "    return y_vocals, mask_vocal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279da80",
   "metadata": {},
   "source": [
    "**BASS & OTHERS EXTRACTION**\n",
    "\n",
    "For bass and other non-vocal components, the process begins with isolating the non-vocal harmonic residue by subtracting the estimated vocal contribution from the harmonic component. Frequency information is then used to create masks: one that selects frequencies below a certain threshold (250 Hz) to capture bass elements, and a complementary mask that selects the remaining frequencies for other elements. These masks are applied to the non-vocal residue, and by incorporating the original phase data, two distinct time-domain signals are reconstructed—one corresponding to the bass and the other to the rest of the non-vocal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8312908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bass and Other extraction\n",
    "def bass_other_extraction(S_harmonic, mask_vocal, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract bass and other components from a mixture using a simple thresholding method.\n",
    "    \n",
    "    Parameters:\n",
    "        S_harmonic (ndarray): The harmonic component of the mixture.\n",
    "        mask_vocal (ndarray): The vocal mask obtained from the vocal extraction.\n",
    "        n_fft (int): FFT window size.\n",
    "        hop_length (int): Hop length for STFT.  \n",
    "    Outputs:\n",
    "        y_bass (ndarray): Extracted bass component.\n",
    "        y_other (ndarray): Extracted other component.   \n",
    "    \"\"\"\n",
    "    # Residuo armonico non-vocale\n",
    "    S_resid = S_harmonic - (mask_vocal * S_harmonic)\n",
    "\n",
    "    # Opzione A: filtro passa-basso per il basso (< 250 Hz)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=n_fft)\n",
    "    bass_mask = (freqs[:, None] < 250.0).astype(float)\n",
    "    other_mask = 1.0 - bass_mask\n",
    "\n",
    "    y_bass  = librosa.istft((S_resid * bass_mask) * phase, hop_length=hop_length, window=win)\n",
    "    y_other = librosa.istft((S_resid * other_mask) * phase, hop_length=hop_length, window=win)\n",
    "\n",
    "    return y_bass, y_other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c826f9",
   "metadata": {},
   "source": [
    "Now we are going to do the separation for just the first track of the dataset to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7217de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# Perform extraction for the first mixture and listen to the results\n",
    "mixture_path = mixture_files[0]\n",
    "mixture, sr = librosa.load(mixture_path, sr=None)\n",
    "\n",
    "\n",
    "# Select the first STFT result\n",
    "mag_first_track = S_full_list[0]\n",
    "phase_first_track = phase_list[0]\n",
    "\n",
    "# Extract drums\n",
    "y_drums, S_harmonic = drums_extraction(mag_first_track, phase_first_track, n_fft=n_fft, hop_length=hop_length)\n",
    "# Extract vocals\n",
    "y_vocals, mask_vocal = vocal_extraction(mag_first_track, phase_first_track, n_fft=n_fft, hop_length=hop_length)\n",
    "# Extract bass and other\n",
    "y_bass, y_other = bass_other_extraction(S_harmonic, mask_vocal, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "# Listen to the results\n",
    "\n",
    "print(\"Drums:\")\n",
    "display(ipd.Audio(y_drums, rate=sr))\n",
    "print(\"Vocals:\")\n",
    "display(ipd.Audio(y_vocals, rate=sr))\n",
    "print(\"Bass:\")\n",
    "display(ipd.Audio(y_bass, rate=sr))\n",
    "print(\"Other:\")\n",
    "display(ipd.Audio(y_other, rate=sr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1753369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# Load the first mixture of the dataset and play it\n",
    "mixture, sr = librosa.load(mixture_path, sr=None)\n",
    "print(\"Mixture:\")\n",
    "ipd.Audio(mixture, rate=sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bb6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hpss_stereo.py\n",
    "# HPSS “from scratch” su segnali stereo\n",
    "\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.ndimage\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def hpss_stereo(\n",
    "    path_in: str,\n",
    "    *,\n",
    "    sr: int = 44_100,\n",
    "    n_fft: int = 4096,\n",
    "    hop: int = 1024,\n",
    "    harmonic_filt: int = 31,\n",
    "    percussive_filt: int = 31,\n",
    "    mask: str = \"soft\",        # \"soft\" oppure \"binary\"\n",
    "    power: float = 2.0,        # =2 ⇒ spettro di potenza; =1 ⇒ magnitudine\n",
    "    out_harm: str | None = None,\n",
    "    out_perc: str | None = None,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    HPSS per file stereo: restituisce due array (shape=(2, n_samples))\n",
    "    contenenti i segnali armonico e percussivo per i canali [L, R].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_in         : percorso del file stereo (wav, flac, mp3,…)\n",
    "    sr              : sample rate di lavoro\n",
    "    n_fft, hop      : parametri STFT\n",
    "    harmonic_filt   : lunghezza filtro orizzontale (frames)\n",
    "    percussive_filt : lunghezza filtro verticale (bin freq)\n",
    "    mask            : \"soft\" o \"binary\"\n",
    "    power           : esponente per lo spettro (1=magn, 2=potenza)\n",
    "    out_harm/out_perc : percorsi opzionali per salvare gli stem stereo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_h, y_p : np.ndarray\n",
    "        Array shape=(2, n_samples): [canale_L, canale_R] separati.\n",
    "    \"\"\"\n",
    "    # 1. carica stereo\n",
    "    y, _ = librosa.load(path_in, sr=sr, mono=False)  # shape (2, n)\n",
    "    if y.ndim == 1:\n",
    "        # se mono, duplichiamo per mantenere shape coerente\n",
    "        y = np.vstack([y, y])\n",
    "    n_channels, n_samples = y.shape\n",
    "\n",
    "    # container per gli stem\n",
    "    y_h = np.zeros_like(y)\n",
    "    y_p = np.zeros_like(y)\n",
    "\n",
    "    # 2–6. per ciascun canale, esegui la pipeline HPSS\n",
    "    for ch in range(n_channels):\n",
    "        X = librosa.stft(y[ch], n_fft=n_fft, hop_length=hop, window=\"hann\")\n",
    "        mag = np.abs(X)\n",
    "        Y = mag**power\n",
    "\n",
    "        # median filtering anisotropo\n",
    "        Y_h = scipy.ndimage.median_filter(Y, size=(1, harmonic_filt))\n",
    "        Y_p = scipy.ndimage.median_filter(Y, size=(percussive_filt, 1))\n",
    "\n",
    "        # creazione maschere\n",
    "        if mask == \"binary\":\n",
    "            Mh = (Y_h >= Y_p).astype(np.float32)\n",
    "            Mp = 1.0 - Mh\n",
    "        else:  # soft\n",
    "            eps = 1e-10\n",
    "            denom = Y_h + Y_p + eps\n",
    "            Mh = Y_h / denom\n",
    "            Mp = Y_p / denom\n",
    "\n",
    "        # applicazione maschere e ricostruzione\n",
    "        X_h = X * Mh\n",
    "        X_p = X * Mp\n",
    "        y_h[ch] = librosa.istft(X_h, hop_length=hop, window=\"hann\", length=n_samples)\n",
    "        y_p[ch] = librosa.istft(X_p, hop_length=hop, window=\"hann\", length=n_samples)\n",
    "\n",
    "    return y_h, y_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and save the two components\n",
    "harmonic_comp, percussive_comp = hpss_stereo(\n",
    "    path_in=mixture_path,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop=512,\n",
    "    harmonic_filt=31,\n",
    "    percussive_filt=31,\n",
    "    mask=\"soft\",\n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Percussive Component:\")\n",
    "print(percussive_comp.shape)\n",
    "display(ipd.Audio(percussive_comp, rate=44100))\n",
    "print(\"Harmonic Component:\")\n",
    "display(ipd.Audio(harmonic_comp, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3739baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmonic_comp, percussive_comp = hpss_stereo(\n",
    "    path_in=mixture_path,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop=512,\n",
    "    harmonic_filt=51,\n",
    "    percussive_filt=101,\n",
    "    mask=\"soft\",\n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Percussive Component:\")\n",
    "print(percussive_comp.shape)\n",
    "display(ipd.Audio(percussive_comp, rate=44100))\n",
    "print(\"Harmonic Component:\")\n",
    "display(ipd.Audio(harmonic_comp, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c98459",
   "metadata": {},
   "source": [
    "**REPET_SIM VOCAL EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import scipy.ndimage\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "def repet_separate_stereo(\n",
    "    y: str,\n",
    "    *,\n",
    "    sr: int = 44100,\n",
    "    n_fft: int = 2048,\n",
    "    hop_length: int = 512,\n",
    "    mask_type: str = \"soft\",  # \"soft\" or \"binary\"\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Separate vocals and music from a stereo mix using the REPET algorithm,\n",
    "    preserving stereo image by computing mask on the mid-channel and\n",
    "    applying smoothed channel-specific masks to L and R.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_in : str\n",
    "        Path to input stereo audio file.\n",
    "    sr : int\n",
    "        Sampling rate for loading and processing.\n",
    "    n_fft : int\n",
    "        FFT window size for STFT.\n",
    "    hop_length : int\n",
    "        Hop length between STFT frames.\n",
    "    mask_type : {\"soft\", \"binary\"}\n",
    "        Type of mask: \"soft\" for proportional, \"binary\" for hard.\n",
    "    out_vocal : str | None\n",
    "        Optional path to save separated vocal track (WAV), stereo.\n",
    "    out_music : str | None\n",
    "        Optional path to save separated music track (WAV), stereo.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocals : np.ndarray\n",
    "        Separated vocal signal, shape=(2, n_samples).\n",
    "    music  : np.ndarray\n",
    "        Separated music signal, shape=(2, n_samples).\n",
    "    \"\"\"\n",
    "    # 1. Load stereo audio\n",
    "    # y, _ = librosa.load(path_in, sr=sr, mono=False)\n",
    "    #display(ipd.Audio(y, rate=sr))\n",
    "    if y.ndim == 1:\n",
    "        y = np.vstack([y, y])\n",
    "    n_ch, n_samples = y.shape\n",
    "\n",
    "    # 2. Compute mid-channel for mask estimation\n",
    "    y_mid = np.mean(y, axis=0)\n",
    "\n",
    "    # 3. STFT on mid-channel\n",
    "    X_mid = librosa.stft(y_mid, n_fft=n_fft, hop_length=hop_length, window='hann', center=True)\n",
    "    V_mid = np.abs(X_mid) ** 2\n",
    "\n",
    "    # 4. Estimate repeating period using onset envelope\n",
    "    onset_env = librosa.onset.onset_strength(y=y_mid, sr=sr, hop_length=hop_length)\n",
    "    tempos = librosa.beat.tempo(onset_envelope=onset_env, sr=sr, hop_length=hop_length)\n",
    "    if len(tempos) == 0:\n",
    "        raise ValueError(\"Could not estimate tempo. Check audio quality.\")\n",
    "    tempo = float(tempos[0])\n",
    "    period = int(round(60 * sr / (tempo * hop_length)))\n",
    "    if period < 1:\n",
    "        raise ValueError(\"Estimated period too small. Check audio or parameters.\")\n",
    "    print(f\"Estimated tempo: {tempo:.2f} BPM, repeating period: {period} frames\")\n",
    "\n",
    "    # 5. Build repeating pattern W\n",
    "    n_bins, n_frames = V_mid.shape\n",
    "    n_segments = n_frames // period\n",
    "    if n_segments < 2:\n",
    "        raise ValueError(\"Not enough repeating segments. Try a smaller period or longer audio.\")\n",
    "    V_trim = V_mid[:, :n_segments * period]\n",
    "    segments = V_trim.reshape(n_bins, period, n_segments)\n",
    "    W = np.median(segments, axis=2)\n",
    "\n",
    "    # 6. Expand W to full length\n",
    "    W_full = np.tile(W, (1, n_segments))\n",
    "    tail = n_frames - W_full.shape[1]\n",
    "    if tail > 0:\n",
    "        W_full = np.hstack((W_full, W[:, :tail]))\n",
    "\n",
    "    # 7. Separate per channel with smoothing\n",
    "    music = np.zeros((n_ch, n_samples))\n",
    "    vocals = np.zeros((n_ch, n_samples))\n",
    "    eps = 1e-10\n",
    "    for ch in range(n_ch):\n",
    "        # STFT per channel\n",
    "        X = librosa.stft(y[ch], n_fft=n_fft, hop_length=hop_length, window='hann', center=True)\n",
    "        V = np.abs(X)**2\n",
    "\n",
    "        # Mask creation per channel\n",
    "        if mask_type == \"binary\":\n",
    "            M = (V >= W_full).astype(float)\n",
    "        else:\n",
    "            M = W_full / (V + eps)\n",
    "        M = np.clip(M, 0, 1)\n",
    "\n",
    "        # Smoothing mask to reduce artefacts\n",
    "        mask_amp = np.sqrt(M)\n",
    "        mask_amp = scipy.ndimage.median_filter(mask_amp, size=(3, 3))\n",
    "\n",
    "        # Apply mask and reconstruct music\n",
    "        X_music = X * mask_amp\n",
    "        y_music = librosa.istft(X_music, hop_length=hop_length, window='hann', center=True, length=n_samples)\n",
    "        music[ch] = y_music\n",
    "        vocals[ch] = y[ch] - y_music\n",
    "\n",
    "\n",
    "    return vocals, music\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal, music = repet_separate_stereo(\n",
    "    y=mixture,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    mask_type=\"soft\"\n",
    ")\n",
    "other = mixture - vocal - music\n",
    "print(\"Mixture:\")\n",
    "display(ipd.Audio(mixture, rate=44100))\n",
    "print(\"Difference between original and music component:\")\n",
    "display(ipd.Audio(other, rate=44100))\n",
    "\n",
    "# output_path = Path(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE\")\n",
    "# output_other= output_path / \"other.wav\"\n",
    "# sf.write(output_other, other.T, 44100)\n",
    "\n",
    "# _, drums = hpss_stereo(\n",
    "#     path_in=output_other,\n",
    "#     sr=44100,\n",
    "#     n_fft=2048,\n",
    "#     hop=512,\n",
    "#     harmonic_filt=31,\n",
    "#     percussive_filt=31,\n",
    "#     mask=\"soft\",\n",
    "#     power=2.0\n",
    "# )\n",
    "\n",
    "# print(\"Drums Component:\")\n",
    "\n",
    "# display(ipd.Audio(drums, rate=44100))\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Vocal Component:\")\n",
    "display(ipd.Audio(vocal, rate=44100))\n",
    "print(\"Music Component:\")\n",
    "display(ipd.Audio(music, rate=44100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780b218",
   "metadata": {},
   "source": [
    "HPSS+REPET-SIM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and save the two components\n",
    "harmonic_comp, percussive_comp = hpss_stereo(\n",
    "    path_in=mixture_path,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop=512,\n",
    "    harmonic_filt=31,\n",
    "    percussive_filt=31,\n",
    "    mask=\"soft\",\n",
    "    power=2.0\n",
    ")\n",
    "\n",
    "# Save harmonic component in this folder\n",
    "output_path = Path(\"/Users/alessandromanattini/Desktop/MAE/SELECTED TOPIC/PROJECT STMAE/musdb18hq_trimmed\")\n",
    "output_harmonic = output_path / \"harmonic.wav\"\n",
    "sf.write(output_harmonic, harmonic_comp.T, 44100)\n",
    "\n",
    "vocal, music = repet_separate_stereo(\n",
    "    path_in=output_harmonic,\n",
    "    sr=44100,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    mask_type=\"soft\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Play the separated components\n",
    "print(\"Percussive Component:\")\n",
    "\n",
    "display(ipd.Audio(percussive_comp, rate=44100))\n",
    "print(\"Vocal Component:\")\n",
    "display(ipd.Audio(vocal, rate=44100))\n",
    "print(\"Music Component:\")\n",
    "display(ipd.Audio(music, rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ce61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
