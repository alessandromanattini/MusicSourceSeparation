{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHplijoPXlMX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import musdb\n",
        "import museval\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# STFT parameters optimized for speed\n",
        "SAMPLE_RATE = 44100\n",
        "N_FFT = 2048  # Reduced from 4096 for faster computation\n",
        "HOP_LENGTH = 512  # Reduced from 1024\n",
        "N_BINS = N_FFT // 2 + 1\n",
        "CHUNK_DURATION = 3  # Reduced from 6 seconds for more iterations\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 20  # Adjust based on training time\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "class SimplifiedOpenUnmix(nn.Module):\n",
        "    \"\"\"Simplified Open-Unmix with 2-layer LSTM for faster training\"\"\"\n",
        "\n",
        "    def __init__(self, n_fft=2048, hidden_size=256, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.n_bins = n_fft // 2 + 1\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input normalization\n",
        "        self.input_norm = nn.BatchNorm1d(self.n_bins)\n",
        "\n",
        "        # Frequency compression (reduce dimensionality)\n",
        "        self.freq_encoder = nn.Linear(self.n_bins, hidden_size)\n",
        "\n",
        "        # Bidirectional LSTM (reduced to 2 layers)\n",
        "        self.lstm = nn.LSTM(\n",
        "            hidden_size,\n",
        "            hidden_size // 2,  # Smaller hidden size\n",
        "            n_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.freq_decoder = nn.Linear(hidden_size, self.n_bins)\n",
        "\n",
        "        # Output activation (sigmoid for mask)\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, time, freq)\n",
        "        # batch, time, freq = x.shape\n",
        "\n",
        "        # Normalize per frequency bin\n",
        "        x = x.transpose(1, 2)  # (batch, freq, time)\n",
        "        x = self.input_norm(x)\n",
        "        x = x.transpose(1, 2)  # (batch, time, freq)\n",
        "\n",
        "        # Frequency encoding\n",
        "        x = self.freq_encoder(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # LSTM processing\n",
        "        x, _ = self.lstm(x)\n",
        "\n",
        "        # Decode back to frequency dimension\n",
        "        x = self.freq_decoder(x)\n",
        "\n",
        "        # Apply sigmoid to get mask values [0, 1]\n",
        "        mask = self.output_activation(x)\n",
        "\n",
        "        return mask\n",
        "\n",
        "class MUSDBDataset(Dataset):\n",
        "    \"\"\"Fast data loader for MUSDB18-HQ\"\"\"\n",
        "\n",
        "    def __init__(self, musdb_root, subset='train', target='vocals',\n",
        "                 chunk_duration=CHUNK_DURATION, sample_rate=SAMPLE_RATE):\n",
        "        self.db = musdb.DB(root=musdb_root, subsets=subset, is_wav=True)\n",
        "        self.target = target\n",
        "        self.chunk_duration = chunk_duration\n",
        "        self.sample_rate = sample_rate\n",
        "        self.chunk_samples = int(chunk_duration * sample_rate)\n",
        "\n",
        "        # Pre-compute valid track indices and lengths for faster sampling\n",
        "        self.track_lengths = []\n",
        "        for track in self.db:\n",
        "            self.track_lengths.append(len(track.audio))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return a fixed number for epoch size\n",
        "        return len(self.db) * 20  # 20 chunks per track per epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Random track selection\n",
        "        track_idx = np.random.randint(len(self.db))\n",
        "        track = self.db[track_idx]\n",
        "\n",
        "        # Random chunk extraction\n",
        "        track_length = self.track_lengths[track_idx]\n",
        "        if track_length > self.chunk_samples:\n",
        "            start = np.random.randint(0, track_length - self.chunk_samples)\n",
        "            end = start + self.chunk_samples\n",
        "        else:\n",
        "            start = 0\n",
        "            end = track_length\n",
        "\n",
        "        # Get mixture and target\n",
        "        mixture = track.audio[start:end].T  # (2, samples)\n",
        "        target_audio = track.targets[self.target].audio[start:end].T\n",
        "\n",
        "        # Convert to mono for simplicity\n",
        "        mixture = mixture.mean(axis=0)\n",
        "        target_audio = target_audio.mean(axis=0)\n",
        "\n",
        "        # Pad if necessary\n",
        "        if len(mixture) < self.chunk_samples:\n",
        "            pad_len = self.chunk_samples - len(mixture)\n",
        "            mixture = np.pad(mixture, (0, pad_len))\n",
        "            target_audio = np.pad(target_audio, (0, pad_len))\n",
        "\n",
        "        return mixture.astype(np.float32), target_audio.astype(np.float32)\n",
        "\n",
        "def compute_stft(audio, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    \"\"\"Compute magnitude spectrogram\"\"\"\n",
        "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
        "    magnitude = np.abs(stft)\n",
        "    phase = np.angle(stft)\n",
        "    return magnitude.T, phase.T  # (time, freq)\n",
        "\n",
        "def apply_mask_and_istft(magnitude, phase, mask, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    \"\"\"Apply mask and convert back to audio\"\"\"\n",
        "    masked_magnitude = magnitude * mask\n",
        "    stft = masked_magnitude.T * np.exp(1j * phase.T)\n",
        "    audio = librosa.istft(stft, hop_length=hop_length)\n",
        "    return audio\n",
        "\n",
        "def evaluate_separation(estimated, reference, sample_rate=SAMPLE_RATE):\n",
        "    \"\"\"Compute SDR, SIR, SAR metrics using museval\"\"\"\n",
        "    # Ensure 2D arrays for museval\n",
        "    if estimated.ndim == 1:\n",
        "        estimated = estimated[np.newaxis, :]\n",
        "    if reference.ndim == 1:\n",
        "        reference = reference[np.newaxis, :]\n",
        "\n",
        "    try:\n",
        "        sdr, isr, sir, sar = museval.evaluate(reference, estimated,\n",
        "                                              win=sample_rate, hop=sample_rate)\n",
        "        return {\n",
        "            'SDR': np.median(sdr),\n",
        "            'SIR': np.median(sir),\n",
        "            'SAR': np.median(sar)\n",
        "        }\n",
        "    except:\n",
        "        return {'SDR': -np.inf, 'SIR': -np.inf, 'SAR': -np.inf}\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    \"\"\"Train one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for mixture, target in tqdm(dataloader, desc=\"Training\"):\n",
        "        mixture = mixture.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Compute spectrograms\n",
        "        batch_size = mixture.shape[0]\n",
        "        mix_mags, phases = [], []\n",
        "        tgt_mags = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            mix_mag, phase = compute_stft(mixture[i].cpu().numpy())\n",
        "            tgt_mag, _ = compute_stft(target[i].cpu().numpy())\n",
        "\n",
        "            mix_mags.append(mix_mag)\n",
        "            phases.append(phase)\n",
        "            tgt_mags.append(tgt_mag)\n",
        "\n",
        "        # Stack and convert to tensors\n",
        "        mix_mags = torch.FloatTensor(np.stack(mix_mags)).to(device)\n",
        "        tgt_mags = torch.FloatTensor(np.stack(tgt_mags)).to(device)\n",
        "\n",
        "        # Normalize magnitude\n",
        "        mix_mags_norm = mix_mags + 1e-8\n",
        "\n",
        "        # Forward pass\n",
        "        masks = model(mix_mags_norm)\n",
        "\n",
        "        # Apply masks\n",
        "        estimated_mags = mix_mags * masks\n",
        "\n",
        "        # MSE loss in magnitude domain\n",
        "        loss = F.mse_loss(estimated_mags, tgt_mags)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Validate and compute metrics\"\"\"\n",
        "    model.eval()\n",
        "    metrics = {'SDR': [], 'SIR': [], 'SAR': []}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for mixture, target in tqdm(dataloader, desc=\"Validating\"):\n",
        "            mixture = mixture.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            batch_size = mixture.shape[0]\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                # Compute spectrograms\n",
        "                mix_mag, phase = compute_stft(mixture[i].cpu().numpy())\n",
        "\n",
        "                # Get mask prediction\n",
        "                mix_mag_tensor = torch.FloatTensor(mix_mag).unsqueeze(0).to(device)\n",
        "                mask = model(mix_mag_tensor + 1e-8).squeeze(0).cpu().numpy()\n",
        "\n",
        "                # Apply mask and reconstruct\n",
        "                estimated = apply_mask_and_istft(mix_mag, phase, mask)\n",
        "                reference = target[i].cpu().numpy()\n",
        "\n",
        "                # Compute metrics\n",
        "                scores = evaluate_separation(estimated, reference)\n",
        "                for key in metrics:\n",
        "                    if not np.isinf(scores[key]):\n",
        "                        metrics[key].append(scores[key])\n",
        "\n",
        "    # Average metrics\n",
        "    avg_metrics = {k: np.mean(v) if v else -np.inf for k, v in metrics.items()}\n",
        "    return avg_metrics\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training loop\"\"\"\n",
        "    # Setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = SimplifiedOpenUnmix().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
        "\n",
        "    # Setup data (update path to your MUSDB18-HQ location)\n",
        "    musdb_root = \"/path/to/musdb18hq\"  # UPDATE THIS PATH\n",
        "\n",
        "    train_dataset = MUSDBDataset(musdb_root, subset='train', target='vocals')\n",
        "    val_dataset = MUSDBDataset(musdb_root, subset='test', target='vocals')\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                            shuffle=True, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4,\n",
        "                          shuffle=False, num_workers=2)\n",
        "\n",
        "    # Training loop\n",
        "    best_sdr = -np.inf\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "        print(f\"Training loss: {train_loss:.4f}\")\n",
        "\n",
        "        # Validate every 2 epochs to save time\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            metrics = validate(model, val_loader, device)\n",
        "            print(f\"Validation metrics - SDR: {metrics['SDR']:.2f} dB, \"\n",
        "                  f\"SIR: {metrics['SIR']:.2f} dB, SAR: {metrics['SAR']:.2f} dB\")\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(metrics['SDR'])\n",
        "\n",
        "            # Save best model\n",
        "            if metrics['SDR'] > best_sdr:\n",
        "                best_sdr = metrics['SDR']\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'metrics': metrics,\n",
        "                }, f'best_model_vocals.pth')\n",
        "                print(f\"Saved best model with SDR: {best_sdr:.2f} dB\")\n",
        "\n",
        "    print(f\"\\nTraining complete! Best SDR: {best_sdr:.2f} dB\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA2tHI_GX1ra"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from scipy.signal import wiener\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import the model class from training script\n",
        "from train_simple_openunmix import SimplifiedOpenUnmix, compute_stft, N_FFT, HOP_LENGTH\n",
        "\n",
        "def load_audio(path, sr=44100, mono=True):\n",
        "    \"\"\"Load audio file\"\"\"\n",
        "    audio, _ = librosa.load(path, sr=sr, mono=mono)\n",
        "    return audio\n",
        "\n",
        "def separate_audio(audio, model, device, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
        "                  chunk_size=44100*10):  # 10 second chunks\n",
        "    \"\"\"Separate audio using trained model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Process in chunks for long audio\n",
        "    separated_chunks = []\n",
        "\n",
        "    for start in range(0, len(audio), chunk_size):\n",
        "        end = min(start + chunk_size, len(audio))\n",
        "        chunk = audio[start:end]\n",
        "\n",
        "        # Compute STFT\n",
        "        magnitude, phase = compute_stft(chunk, n_fft, hop_length)\n",
        "\n",
        "        # Prepare input\n",
        "        mag_tensor = torch.FloatTensor(magnitude).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Get mask prediction\n",
        "            mask = model(mag_tensor + 1e-8).squeeze(0).cpu().numpy()\n",
        "\n",
        "            # Apply mask\n",
        "            separated_magnitude = magnitude * mask\n",
        "\n",
        "            # Convert back to audio\n",
        "            stft_separated = separated_magnitude.T * np.exp(1j * phase.T)\n",
        "            separated_chunk = librosa.istft(stft_separated, hop_length=hop_length)\n",
        "\n",
        "        separated_chunks.append(separated_chunk)\n",
        "\n",
        "    # Concatenate all chunks\n",
        "    separated = np.concatenate(separated_chunks)\n",
        "\n",
        "    return separated\n",
        "\n",
        "def post_process_wiener(separated, mixture, n_fft=N_FFT, hop_length=HOP_LENGTH):\n",
        "    \"\"\"Apply Wiener filtering for better quality\"\"\"\n",
        "    # Simple Wiener filter implementation\n",
        "    stft_mix = librosa.stft(mixture, n_fft=n_fft, hop_length=hop_length)\n",
        "    stft_sep = librosa.stft(separated, n_fft=n_fft, hop_length=hop_length)\n",
        "\n",
        "    # Estimate noise as mixture - separated\n",
        "    noise_estimate = np.abs(stft_mix) - np.abs(stft_sep)\n",
        "    noise_estimate = np.maximum(noise_estimate, 0)\n",
        "\n",
        "    # Apply Wiener filter\n",
        "    wiener_mask = np.abs(stft_sep)**2 / (np.abs(stft_sep)**2 + noise_estimate**2 + 1e-8)\n",
        "    filtered_stft = stft_mix * wiener_mask\n",
        "\n",
        "    # Convert back to audio\n",
        "    filtered_audio = librosa.istft(filtered_stft, hop_length=hop_length)\n",
        "\n",
        "    return filtered_audio\n",
        "\n",
        "def separate_file(input_path, output_path, model_path, target='vocals',\n",
        "                  use_wiener=True, device=None):\n",
        "    \"\"\"Main separation function\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    # Load model\n",
        "    model = SimplifiedOpenUnmix().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Loading audio from {input_path}\")\n",
        "    # Load audio\n",
        "    audio = load_audio(input_path, mono=True)\n",
        "\n",
        "    print(f\"Separating {target}...\")\n",
        "    # Separate\n",
        "    separated = separate_audio(audio, model, device)\n",
        "\n",
        "    if use_wiener:\n",
        "        print(\"Applying Wiener filter post-processing...\")\n",
        "        separated = post_process_wiener(separated, audio)\n",
        "\n",
        "    print(f\"Saving to {output_path}\")\n",
        "    # Save output\n",
        "    sf.write(output_path, separated, 44100)\n",
        "\n",
        "    print(\"Separation complete!\")\n",
        "\n",
        "    return separated\n",
        "\n",
        "def batch_separate(input_dir, output_dir, model_path, target='vocals'):\n",
        "    \"\"\"Separate multiple files\"\"\"\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    input_path = Path(input_dir)\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model once\n",
        "    model = SimplifiedOpenUnmix().to(device)\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    # Process all audio files\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']\n",
        "    audio_files = [f for f in input_path.iterdir()\n",
        "                   if f.suffix.lower() in audio_extensions]\n",
        "\n",
        "    for audio_file in audio_files:\n",
        "        print(f\"\\nProcessing {audio_file.name}\")\n",
        "\n",
        "        try:\n",
        "            # Load audio\n",
        "            audio = load_audio(str(audio_file), mono=True)\n",
        "\n",
        "            # Separate\n",
        "            separated = separate_audio(audio, model, device)\n",
        "\n",
        "            # Post-process\n",
        "            separated = post_process_wiener(separated, audio)\n",
        "\n",
        "            # Save\n",
        "            output_file = output_path / f\"{audio_file.stem}_{target}.wav\"\n",
        "            sf.write(output_file, separated, 44100)\n",
        "\n",
        "            print(f\"Saved to {output_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_file.name}: {e}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Single file separation\n",
        "    separate_file(\n",
        "        input_path=\"path/to/input.wav\",\n",
        "        output_path=\"path/to/output_vocals.wav\",\n",
        "        model_path=\"best_model_vocals.pth\",\n",
        "        target='vocals',\n",
        "        use_wiener=True\n",
        "    )\n",
        "\n",
        "    # Batch processing\n",
        "    # batch_separate(\n",
        "    #     input_dir=\"path/to/input_folder\",\n",
        "    #     output_dir=\"path/to/output_folder\",\n",
        "    #     model_path=\"best_model_vocals.pth\",\n",
        "    #     target='vocals'\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkTZJcQ3X35T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from train_simple_openunmix import SimplifiedOpenUnmix, MUSDBDataset, train_epoch, validate\n",
        "\n",
        "def train_all_stems(musdb_root, max_hours=4, device='cuda'):\n",
        "    \"\"\"Train models for all 4 stems within time budget\"\"\"\n",
        "\n",
        "    stems = ['vocals', 'drums', 'bass', 'other']\n",
        "    time_per_stem = (max_hours * 3600) / len(stems)  # Seconds per stem\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for stem in stems:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Training model for: {stem}\")\n",
        "        print(f\"Time budget: {time_per_stem/3600:.1f} hours\")\n",
        "        print(f\"{'='*50}\\n\")\n",
        "\n",
        "        # Initialize model\n",
        "        model = SimplifiedOpenUnmix().to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
        "\n",
        "        # Setup data\n",
        "        train_dataset = MUSDBDataset(musdb_root, subset='train', target=stem)\n",
        "        val_dataset = MUSDBDataset(musdb_root, subset='test', target=stem)\n",
        "\n",
        "        # Adjust batch size based on available memory\n",
        "        batch_size = 16 if device == 'cuda' else 8\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=4,\n",
        "            pin_memory=(device == 'cuda')\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=4,\n",
        "            shuffle=False,\n",
        "            num_workers=2\n",
        "        )\n",
        "\n",
        "        # Training loop with time constraint\n",
        "        start_time = time.time()\n",
        "        best_sdr = -np.inf\n",
        "        epoch = 0\n",
        "\n",
        "        while (time.time() - start_time) < time_per_stem:\n",
        "            epoch += 1\n",
        "            print(f\"\\nEpoch {epoch} for {stem}\")\n",
        "\n",
        "            # Train\n",
        "            train_loss = train_epoch(model, train_loader, optimizer, device)\n",
        "            print(f\"Training loss: {train_loss:.4f}\")\n",
        "\n",
        "            # Quick validation every 3 epochs\n",
        "            if epoch % 3 == 0:\n",
        "                metrics = validate(model, val_loader, device)\n",
        "                print(f\"Validation - SDR: {metrics['SDR']:.2f} dB\")\n",
        "\n",
        "                scheduler.step(metrics['SDR'])\n",
        "\n",
        "                if metrics['SDR'] > best_sdr:\n",
        "                    best_sdr = metrics['SDR']\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'metrics': metrics,\n",
        "                    }, f'model_{stem}.pth')\n",
        "\n",
        "            # Check time\n",
        "            elapsed = time.time() - start_time\n",
        "            remaining = time_per_stem - elapsed\n",
        "            print(f\"Time elapsed: {elapsed/60:.1f} min, remaining: {remaining/60:.1f} min\")\n",
        "\n",
        "            if remaining < 120:  # Less than 2 minutes left\n",
        "                print(f\"Time limit approaching, stopping training for {stem}\")\n",
        "                break\n",
        "\n",
        "        # Final validation\n",
        "        print(f\"\\nFinal validation for {stem}\")\n",
        "        final_metrics = validate(model, val_loader, device)\n",
        "        results[stem] = final_metrics\n",
        "\n",
        "        print(f\"\\nCompleted {stem} - Final SDR: {final_metrics['SDR']:.2f} dB\")\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(\"TRAINING COMPLETE - SUMMARY\")\n",
        "    print(f\"{'='*50}\")\n",
        "    for stem, metrics in results.items():\n",
        "        print(f\"{stem:8s}: SDR={metrics['SDR']:6.2f} dB, \"\n",
        "              f\"SIR={metrics['SIR']:6.2f} dB, \"\n",
        "              f\"SAR={metrics['SAR']:6.2f} dB\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def create_separation_script():\n",
        "    \"\"\"Create a complete separation script using all trained models\"\"\"\n",
        "\n",
        "    script_content = '''\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from train_simple_openunmix import SimplifiedOpenUnmix, compute_stft, N_FFT, HOP_LENGTH\n",
        "\n",
        "def separate_all_stems(input_path, output_dir, models_dir='.'):\n",
        "    \"\"\"Separate audio into all 4 stems\"\"\"\n",
        "    import os\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Create output directory\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # Load audio\n",
        "    audio, sr = librosa.load(input_path, sr=44100, mono=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    stems = ['vocals', 'drums', 'bass', 'other']\n",
        "\n",
        "    for stem in stems:\n",
        "        print(f\"Separating {stem}...\")\n",
        "\n",
        "        # Load model\n",
        "        model = SimplifiedOpenUnmix().to(device)\n",
        "        model_path = Path(models_dir) / f'model_{stem}.pth'\n",
        "\n",
        "        if not model_path.exists():\n",
        "            print(f\"Model not found: {model_path}\")\n",
        "            continue\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model.eval()\n",
        "\n",
        "        # Separate\n",
        "        magnitude, phase = compute_stft(audio)\n",
        "        mag_tensor = torch.FloatTensor(magnitude).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask = model(mag_tensor + 1e-8).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Apply mask and reconstruct\n",
        "        separated_magnitude = magnitude * mask\n",
        "        stft_separated = separated_magnitude.T * np.exp(1j * phase.T)\n",
        "        separated_audio = librosa.istft(stft_separated, hop_length=HOP_LENGTH)\n",
        "\n",
        "        # Save\n",
        "        output_file = output_path / f'{Path(input_path).stem}_{stem}.wav'\n",
        "        sf.write(output_file, separated_audio, sr)\n",
        "        print(f\"Saved: {output_file}\")\n",
        "\n",
        "    print(\"Separation complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    separate_all_stems(\n",
        "        input_path=\"path/to/input.wav\",\n",
        "        output_dir=\"separated_output\",\n",
        "        models_dir=\".\"\n",
        "    )\n",
        "'''\n",
        "\n",
        "    with open('separate_all_stems.py', 'w') as f:\n",
        "        f.write(script_content)\n",
        "\n",
        "    print(\"Created separation script: separate_all_stems.py\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    MUSDB_ROOT = \"/path/to/musdb18hq\"  # UPDATE THIS\n",
        "    MAX_TRAINING_HOURS = 4\n",
        "\n",
        "    # Check device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    if device.type == 'cpu':\n",
        "        print(\"WARNING: Training on CPU will be slow. Consider using GPU.\")\n",
        "        print(\"Reducing model size and batch size for CPU training...\")\n",
        "\n",
        "    # Train all stems\n",
        "    results = train_all_stems(MUSDB_ROOT, MAX_TRAINING_HOURS, device)\n",
        "\n",
        "    # Create separation script\n",
        "    create_separation_script()\n",
        "\n",
        "    print(\"\\nTraining complete! Use 'separate_all_stems.py' to separate new songs.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
